{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ab7e836",
   "metadata": {
    "id": "7ab7e836"
   },
   "source": [
    "# Linear Regression Theory\n",
    "(by Tevfik Aytekin)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b411dbf7",
   "metadata": {
    "id": "b411dbf7"
   },
   "source": [
    "### Preliminaries and Intuition\n",
    "Suppose that we are given a data set $D = ((x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ..., (x^{(m)}, y^{(m)}))$ \n",
    "\n",
    "where $x^{(i)} \\in \\mathbb{R}^n$ and $y^{(i)} \\in \\mathbb{R}$.\n",
    "\n",
    "We can think of this data as a sample of the inputs and outputs of an unknown function. Our aim is given this data to find the unknown function (or an approximation of it). \n",
    "\n",
    "\n",
    "#### A Simple Example\n",
    "Suppose that we have the following toy dataset.\n",
    "\n",
    "| x       | y        | \n",
    "| ----------- | ----------- |\n",
    "| 4      | 16       | \n",
    "| 3   | 9        |\n",
    "| 8   | 64  | \n",
    "| 7      | 49       | \n",
    "\n",
    "Given this dataset what might be the relationship between $x$ and $y$? Many will answer this as $y = xÂ²$. This simple example illustrates the main idea of regression, machine learning, and even science in general: that is, given some observations find a general pattern that explains the observations. But unfortunately things are more sophisticated. One big issue is the problem of induction which points out that going from a finite set of observations to a general theory is not logically valid. We will not discuss this deep issue here but interested ones can read [Problem of Induction](https://plato.stanford.edu/entries/induction-problem/) and [Underdetermination](https://plato.stanford.edu/entries/scientific-underdetermination/).\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b91998",
   "metadata": {
    "id": "c4b91998"
   },
   "source": [
    "\n",
    "#### A More Realistic Example\n",
    "In a real life application the input and output variables will be from observations of some real life phenomena. And almost always there will be some uncontrolled or unknown variables. For example, the following dataset shows the number of rooms, area, and price of several houses. It only shows 5 houses as examples but in a real problem normally we have much more examples (typically more than 1000). It is clear that there are many other factors (some known, some unknown) which effect house prices. Also the measurements (the values of the variables) might contain errors. Can we find a formula which relates number of rooms and area to the price of a house? How can you approach this problem? Any ideas?\n",
    "\n",
    "| | Rooms       | Area        |  Price |\n",
    "|-| ----------- | ----------- |--------|\n",
    "|House1 | 4      | 120       | 100000|\n",
    "|House2| 3   | 110        | 90000|\n",
    "|House3| 5   | 210  | 150000|\n",
    "|House4| 2   | 140        | 80000|\n",
    "|House5| 4   | 180  | 140000|\n",
    "\n",
    "\n",
    "One reasonable approach might be to assume a linear relationship such as the one given below:\n",
    "\n",
    "$$\n",
    "price = \\theta_0 + \\theta_1 rooms + \\theta_2 area \n",
    "$$\n",
    "\n",
    "If this linear assumption is true what might be some good estimates for the parameters, that is, $\\theta_i$'s. One plausible aim is to find parameter values which minimize the error:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^m (price^{(i)} - (\\theta_0+\\theta_1rooms^{(i)}+\\theta_2area^{(i)}))^2 \n",
    "$$\n",
    "\n",
    "where $m$ is the number of examples. You might ask why are we taking the square of the difference. There are a couple of reasons. One of them is to turn to error into a positive value so that negative and positive values will not cancel each other. You might think why not just take the absolute value. One reason is, the resulting function is easier to differentiate which will be needed as we will discuss. Another reason is that squared error is a logical consequence if we assume that errors are normally distributed which we will discuss next.\n",
    "\n",
    "More formally and generally we can write the function to be minimized (called the cost function) as follows:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^m (y^{(i)} - \\theta^Tx^{(i)})^2 \n",
    "$$\n",
    "\n",
    "where, $y^{(i)}$ is the output value of the $i$th example, $x^{(i)}$ is the $i$th input vector, and $\\theta$ is the parameter vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bda446",
   "metadata": {
    "id": "69bda446"
   },
   "source": [
    "### Probabilistic Interpretation\n",
    "\n",
    "Assumption:\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "y^{(i)}&= \\theta_0x^{(i)}_0+\\theta_1x^{(i)}_1+\\theta_2x^{(i)}_2+...+\\theta_nx^{(i)}_n + \\epsilon^{(i)} \\\\\n",
    "& = \\theta^Tx^{(i)} + \\epsilon^{(i)}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "The major difference in this formulation is the addition of the $\\epsilon^{(i)}$ term which represents some unaccounted effects. We will asume that $\\epsilon^{(i)} \\sim \\mathcal{N}(0,\\,\\sigma^{2})$ which means that the probability density function of $\\epsilon^{(i)}$  is:\n",
    "$$\n",
    "p(\\epsilon^{(i)}) = \\frac{1}{ \\sqrt{2\\pi\\sigma^2 }}\\exp\\left(-\\frac{(\\epsilon^{(i)})^2}{2\\sigma ^2 }\\right)\n",
    "$$\n",
    "\n",
    "Here comes the critical question: given these model assumptions and a dataset, what are the most probable values of the parameters? To answer this, we will first formulate the likelihood of the data given our assumptions. The likelihood function can be formuated as follows:\n",
    "$$\n",
    "p(y^{(i)} | x^{(i)}; \\theta) = \\frac{1}{ \\sqrt{2\\pi\\sigma^2 }}\\exp\\left(-\\frac{(y^{(i)}-\\theta^Tx^{(i)})^2}{2\\sigma ^2 }\\right)\n",
    "$$\n",
    "\n",
    "We can understand the above formula as follows: It is the probability of seeing a $y^{(i)}$ given that we see a $x^{(i)}$ and this probability is the value of the normal distribution at $y^{(i)}-\\theta^Tx^{(i)}$. Actually there is some subtle issue here. This likelihood is not the likelihood of the data. Since we don't know the distributions of $x$ and $y$ we can't write the likelihood of the data. And we don't need to formulate it, because linear regression assumption is an assumption about the relationship between the variables not about their probability of occurrences. Also note that the above likelihood can also be written by conditioning on $y$ and the result will be exactly the same. We can continue the derivation in a similar way.\n",
    "\n",
    "Here is the next step: what is the probability of seeing $n$ number of $y$'s, namely, $y^{(1)}, y^{(2)}, ..., y^{(n)}$ given the corresponding $x$'s, namely, $x^{(1)}, x^{(2)}, ..., x^{(n)}$ . Given that $y^{(i)}$'s are independent (independence assumption) of each other this probability can be written as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "L(\\theta)& =\\prod_{i=1}^m p(y^{(i)}| x^{(i)}; \\theta) \\\\\n",
    "& = \\prod_{i=1}^m \\frac{1}{ \\sqrt{2\\pi\\sigma^2 }}\\exp\\left(\\frac{-(y^{(i)}-\\theta^Tx^{(i)})^2}{2\\sigma ^2 }\\right)\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "The next question is to ask which values of $\\theta$ makes this likelihood most likely (known as the maximum likelihood estimation, for more on mle you can look at [this notebook](mle.ipynb)). Now, we have an optimization problem, find the values $\\theta$ which maximizes $L(\\theta)$.\n",
    "\n",
    "A common trick is to maximize the log of this likelihood which is easier to solve and since log is a strictly increasing function the result will be the same.\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "logL(\\theta) & = log\\prod_{i=1}^m \\frac{1}{ \\sqrt{2\\pi\\sigma^2 }}\\exp\\left(\\frac{-(y^{(i)}-\\theta^Tx^{(i)})^2}{2\\sigma ^2 }\\right) \\\\\n",
    "& = \\sum_{i=1}^m log\\frac{1}{ \\sqrt{2\\pi\\sigma^2 }}\\exp\\left(\\frac{-(y^{(i)}-\\theta^Tx^{(i)})^2}{2\\sigma ^2 }\\right) \\\\\n",
    "& = mlog\\frac{1}{ \\sqrt{2\\pi\\sigma^2 }}-\\frac{1}{\\sigma^2}\\frac{1}{2}\\sum_{i=1}^m (y^{(i)} - \\theta^Tx^{(i)})^2 \n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "As can be seen above, maximizing $logL(\\theta)$ is equivalent to minimizing \n",
    "$$\n",
    "\\sum_{i=1}^m (y^{(i)} - \\theta^Tx^{(i)})^2 \n",
    "$$\n",
    "which is the cost function we defined at the beginning. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e443fc",
   "metadata": {
    "id": "86e443fc"
   },
   "source": [
    "### Batch Gradient Descent\n",
    "<blockquote>\n",
    "<b>Algorithm</b>: Batch Gradient Descent <br>\n",
    "repeat <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $\\theta_j = \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j}J(\\theta) $ <br>\n",
    "until convergence\n",
    "</blockquote>\n",
    "    \n",
    "Below is the derivative of the cost function for a data set where there is a single example ($x, y$).\n",
    "\n",
    "\\begin{equation} \\label{eq1}\n",
    "\\begin{split}\n",
    "\\frac{\\partial}{\\partial \\theta_j}J(\\theta) & =  \\frac{\\partial}{\\partial \\theta_j}\\frac{1}{2}(y-h_\\theta(x))^2 \\\\\n",
    " & =2\\frac{1}{2}(y-h_\\theta(x)) \\frac{\\partial}{\\partial \\theta_j} (y-h_\\theta(x))\\\\\n",
    " & =(y-h_\\theta(x)) \\frac{\\partial}{\\partial \\theta_j}\\left(y- \\sum_{i=0}^{n}\\theta_ix_{i}\\right)\\\\\n",
    "  & =-(y-h_\\theta(x)) x_{j}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "For $m$ examples:\n",
    "\\begin{equation}\n",
    "\\frac{\\partial}{\\partial \\theta_j}J(\\theta) = -\\sum_{i=1}^m(y^{(i)}-h_\\theta(x^{(i)})) x_{j}\n",
    "\\end{equation}\n",
    "\n",
    "So, gradient descent algorithm becomes:\n",
    "\n",
    "<blockquote>\n",
    "<b>Algorithm</b>: Batch Gradient Descent <br>\n",
    "repeat<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $\\theta_j = \\theta_j + \\alpha \\sum\\limits_{i=1}^m(y^{(i)}-h_\\theta(x^{(i)})) x_{j} $ &nbsp;&nbsp;&nbsp;&nbsp; {(for every $j$)} <br>\n",
    "until convergence\n",
    "</blockquote>\n",
    "\n",
    "    \n",
    "$\\alpha$ is called the learning rate which controls the magnitude of the updates. Note that you need to update $\\theta_j$'s simultaneously. \n",
    "\n",
    "### Stochastic Gradient descent\n",
    "\n",
    "<blockquote>\n",
    "<b>Algorithm</b>: Stochastic Gradient Descent <br>\n",
    "repeat <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; shuffle the data <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; for $i = 0$ to $m$ do <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;  $\\theta_j = \\theta_j +\\alpha(y^{(i)}-h_\\theta(x^{(i)})) x_{j} $  &nbsp;&nbsp;&nbsp;&nbsp;   (for every $j$) <br>\n",
    "until convergence\n",
    "</blockquote>\n",
    "    \n",
    "    \n",
    "Different from the batch version stochastic gradient ascent update the parameters after seeing every individual example. Stochastic gradient descent achieves faster convergence than the batch version.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846d8131",
   "metadata": {
    "id": "846d8131"
   },
   "source": [
    "### Closed Form Solution\n",
    "\n",
    "Using vector notation we can write the cost function\n",
    "\\begin{equation}\n",
    "J(\\theta) =  \\sum_{i=1}^m (y^{(i)} - h_\\theta(x^{(i)}))^2 \n",
    "\\end{equation}\n",
    "as follows:\n",
    "\\begin{equation}\n",
    "(y - X\\theta)^T(y-X\\theta)\\\\\n",
    "\\end{equation}\n",
    "\n",
    "In order to find the values of $\\theta$ which minimizes the cost function we need to set the derivative to zero and solve for $\\theta$.\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    " \\nabla (y - X\\theta)^T(y-X\\theta) & = 0 \\\\\n",
    "  -2X^T(y-X\\theta) & = 0 \\\\\n",
    "-2X^Ty+2X^TX\\theta & = 0\\\\\n",
    "(X^TX)^{-1}X^TX\\theta & = (X^TX)^{-1}X^Ty \\\\\n",
    "I\\theta & = (X^TX)^{-1}X^Ty \\\\\n",
    "\\theta & = (X^TX)^{-1}X^Ty\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Note that the time complexity of the matrix inverse operation is $O(d)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027ca07d",
   "metadata": {
    "id": "027ca07d"
   },
   "source": [
    "### Regularized Linear Regression\n",
    "#### Ridge Regression \n",
    "\n",
    "Cost function: <br>\n",
    "\\begin{equation}\n",
    "J(\\theta) = \\displaystyle \\frac{1}{2m} \\left[\\sum\\limits_{i=1}^m (y^{(i)} - h_\\theta(x^{(i)}))^2 + \\lambda\\sum\\limits_{j=1}^n\\theta_j^2\\right] \n",
    "\\end{equation}\n",
    "\n",
    "<b>Algorithm:</b> Gradient Descent for Ridge Regression\n",
    "<blockquote>\n",
    "repeat<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;  $\\theta_0 := \\theta_0 + \\alpha \\frac{1}{m}  \\sum\\limits_{i=1}^m (y^{(i)}-h_\\theta(x^{(i)})x_{0}$ <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;  $\\theta_j := \\theta_j + \\alpha \\left[ \\frac{1}{m}  \\sum\\limits_{i=1}^m (y^{(i)}-h_\\theta(x^{(i)}))x_{j} - \\frac{\\lambda}{m}\\theta_j \\right]$ &nbsp;&nbsp;&nbsp;&nbsp;  $(j = 1,2,3, ..., n)$\n",
    "</blockquote>\n",
    "\n",
    "<b>Closed form solution:</b><br>\n",
    "\\begin{equation}\n",
    "\\theta = (X^TX+\\lambda I)^{-1}X^Ty\n",
    "\\end{equation}\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
